{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aim is to create a model that can predict the final grade (G3) from the rest of the dataset. \n",
    "# ie. G3 is the target label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports & Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings # to remove warning when the model is running\n",
    "\n",
    "from sklearn.model_selection import train_test_split # to split data into train and test sets\n",
    "from sklearn.linear_model import LogisticRegression # to use Logistic Regression for step of stacking \n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier # to use RF and GB as base models + the stacked model\n",
    "from sklearn.metrics import accuracy_score # to get the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"G3\" # the target variable\n",
    "\n",
    "\n",
    "# ONE_BOUND is the bounding area from the extremes (ie. 1 and -1), to remove the models that are too correlated with the target\n",
    "# ZERO_BOUND is the bounding area from the middle (ie. 0), to remove the models that are too uncorrelated with the target\n",
    "ONE_BOUND =0.8 ; ZERO_BOUND = 0.065 \n",
    "\n",
    "TEST_SIZE = 0.2 # percentage of the dataset to be used as a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **The Dataset and Correlation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "portugeseDF = pd.read_csv('../data/Portuguese.csv')\n",
    "\n",
    "# This part is copied from \"Dataset.ipynb\" to get the confusion matrix\n",
    "le = LabelEncoder()\n",
    "for col in portugeseDF.select_dtypes(include=['object']).columns:\n",
    "    portugeseDF[col] = le.fit_transform(portugeseDF[col])\n",
    "    \n",
    "correlation_matrix = portugeseDF.corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
    "correlation_matrix = correlation_matrix.mask(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Different Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two base models for the stacking model\n",
    "randomForestModel = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "gradientBoostingModel = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# stacking model that uses a 5 fold cross validation scheme (cv)\n",
    "stackingEnsembleModel = StackingClassifier(estimators=[(\"random_forest\", randomForestModel), (\"gradient_boosting\", gradientBoostingModel)], final_estimator=LogisticRegression(), cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Functions to Create New Dataset and Run the Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makingNewDataset(correlationMatrix, dataframe):\n",
    "    orignalAttributes = dict(correlationMatrix.loc[TARGET])\n",
    "    newAttributes = list()\n",
    "    removedAttributes = list()\n",
    "\n",
    "    # iterate through features to choose which to keep\n",
    "    for key in orignalAttributes:\n",
    "        if key == TARGET: newAttributes.append(key) # add target to new dataframe\n",
    "        elif orignalAttributes[key] > ONE_BOUND or orignalAttributes[key] < -(ONE_BOUND): removedAttributes.append(key) # high correlation\n",
    "        elif orignalAttributes[key] < ZERO_BOUND and orignalAttributes[key] > -(ZERO_BOUND): removedAttributes.append(key) # low correlation\n",
    "        else: newAttributes.append(key) # add featuers that are in the acceptable range to new dataframe\n",
    "    \n",
    "    # make new dataframe\n",
    "    newData = {}\n",
    "    for attribute in newAttributes:\n",
    "        newData[attribute] = dataframe[attribute]\n",
    "    newPortugeseDF = pd.DataFrame(newData)\n",
    "\n",
    "    return [newPortugeseDF, newAttributes, removedAttributes] # return new dataframe, list of kept attributes and list of removed attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTheModel (dataframe, modelName, modelToRun):\n",
    "    features = dataframe.drop(columns=[TARGET]); target = dataframe[TARGET]\n",
    "    featureTrain, featureTest, targetTrain, targetTest = train_test_split(features, target, test_size=TEST_SIZE, random_state=42)\n",
    "\n",
    "    modelToRun.fit(featureTrain, targetTrain)\n",
    "\n",
    "    testPredictions = modelToRun.predict(featureTest)\n",
    "    accuracy = accuracy_score(targetTest, testPredictions)\n",
    "    print(f\"{modelName} Accuracy: \\t{accuracy * 100:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation values in range: 0.8 to 0.065 and -0.065 to -0.8\n",
      "\n",
      "List Kept Features:\t\t\t['school', 'sex', 'age', 'address', 'Medu', 'Fedu', 'Mjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'higher', 'internet', 'romantic', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences', 'G3']\n",
      "List Removed Features:\t\t\t['famsize', 'Pstatus', 'Fjob', 'famsup', 'paid', 'activities', 'nursery', 'famrel', 'G1', 'G2']\n",
      "Original / Removed / Kept:\t\t33 / 10 / 23\n"
     ]
    }
   ],
   "source": [
    "print(f\"Correlation values in range: {ONE_BOUND} to {ZERO_BOUND} and -{ZERO_BOUND} to -{ONE_BOUND}\\n\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning); warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "    temp=makingNewDataset(correlation_matrix, portugeseDF); newDataset=temp[0]; keptAttributes=temp[1]; removedAttributes=temp[2]\n",
    "    print(f\"List Kept Features:\\t\\t\\t{keptAttributes}\"); print(f\"List Removed Features:\\t\\t\\t{removedAttributes}\")\n",
    "    print(f\"Original / Removed / Kept:\\t\\t{len(portugeseDF.keys())} / {len(removedAttributes)} / {len(keptAttributes)}\")\n",
    "\n",
    "    runTheModel(newDataset, \"\\nStacking Ensemble Model\", stackingEnsembleModel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AppliedMachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
